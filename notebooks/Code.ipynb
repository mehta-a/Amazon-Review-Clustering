{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a coding exercise meant to test your ability to understand and implement basic concepts. Please read all the points below BEFORE starting the exercise.\n",
    "\n",
    "\n",
    "**1) Implement well-commented functions with docstrings in python3 to perform the following (see this video for reference: https://www.youtube.com/watch?v=xKaoGeKD43w):**\n",
    "\n",
    "- a) Read a text file using the path to the text file as the input parameter.\n",
    "- b) Remove punctuations, numbers and special characters given a document as input. \n",
    "- c) Remove stopwords given a document as the input parameter - a basic list of stopwords is provided, you can use more extensive lists from the internet (please cite the link in your comments if you do).\n",
    "- d) Apply stemming or lemmatisation given a document as the input parameter (functionality from any pre-existing package can be used to do this).\n",
    "- e) Extract n-grams given a document as the input parameter (functionality from any pre-existing package can be used to do this).\n",
    "- f) Create a tf-idf matrix from scratch given a corpus as the input parameter (do not use any pre-exisiting packages that offer functionality to do this directly).\n",
    "- g) Perform k-means clustering on the rows of a matrix, given k and the matrix whose rows are to be clustered as input parameters (implement k-means from scratch and do not use any pre-existing packages that offer functionality to do this directly). \n",
    "\n",
    "**2) Write tests for each of the functions above using the Test Corpus provided - assume each new line is a separate document.**\n",
    "\n",
    "**3) Each of the 2 Main Corpora provided comprises of product reviews from Amazon.com for a pair of products each. In both corpora, the reviews for each product are separated by three blank lines. In a Jupyter notebook, use your functions 1 a)-g) to treat and cluster the documents in both the Main Corpora.**\n",
    "\n",
    "- a) In both cases use k=2 and assume each new line is a separate document. \n",
    "- b) Create a 'confusion matrix' A per corpus, whose rows are 1st and 2nd product, whose columns are 1st and 2nd cluster and whose entries Aij are the count of reviews of product i in \t\t\tcluster j.\n",
    "- c) Compare the clustering results for both corpora and if you feel the results can be improved, how this can be achieved. \n",
    "\n",
    "**4) In a Jupyter notebook, cluster the provided data (Clustering Data) using your k-means implementation. The data cannot be transformed in any way apart from mean-centering.**\n",
    "\n",
    "- a) Assess the performance of this k-means implementation for this dataset\n",
    "- b) Discuss the performance and if there is scope for improvement, propose changes to your k-means implementation that would help improve results and explain why  \n",
    "\n",
    "You will need to submit ALL your code(s) for 1) and 2), and notebooks for 3) and 4) above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Implement the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import string # Require for clean text\n",
    "import math # Require for taking log\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. a) Read a text file using the path to the text file as the input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in consideration\n",
    "text_file_1 = 'Main Corpus 1/Main Corpus 1.txt'\n",
    "text_file_2 = 'Main Corpus 2/Main Corpus 2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_lines(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Works great, no odor, and uses regular bags.\\n', \"Can't complain at all!\\n\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_1 = get_file_lines(text_file_1)\n",
    "lines_1[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. b) Remove punctuations, numbers and special characters given a document as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = ''.join([x for x in text if x not in [*string.punctuation, *string.digits, *['\\n','\\r','\\t']]])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Works great no odor and uses regular bags', 'Cant complain at all']\n"
     ]
    }
   ],
   "source": [
    "temp = list(map(clean_text, clean_list(lines_1[:2])))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. c) Remove stopwords given a document as the input parameter - a basic list of stopwords is provided, you can use more extensive lists from the internet (please cite the link in your comments if you do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(stopwords_file, text_to_clean):\n",
    "    stp_words = get_file_lines(stopwords_file)\n",
    "    # Remove punctuation, digits, or extra special characters from stopwords file.\n",
    "    stp_words = list(map(clean_text, stp_words))\n",
    "    resp = ' '.join([x for x in text_to_clean.split(' ') if x not in stp_words])\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Works great odor uses regular bags'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_file = 'Stopwords/Basic Stopwords List.txt'\n",
    "remove_stopwords(stop_words_file, clean_text(lines_1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. d) Apply stemming or lemmatisation given a document as the input parameter (functionality from any pre-existing package can be used to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(doc):\n",
    "    stemed_list = [stemmer.stem(x) for x in doc.split()]\n",
    "    stemmed_doc = ' '.join([x for x in stemed_list])\n",
    "    return stemmed_doc\n",
    "def apply_lemmatization(doc):\n",
    "    lemma_list = [lemmatizer.lemmatize(x, pos='v') for x in doc.split()]\n",
    "    lemma_doc = ' '.join([x for x in lemma_list])\n",
    "    return lemma_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_doc = 'Works great no odor and uses regular bags for us in United States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work great no odor and use regular bag for us in unit state'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_stemming(ex_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Works great no odor and use regular bag for us in United States'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_lemmatization(ex_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work great no odor and use regular bag for us in unit state'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_lemmatization(apply_stemming(ex_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. e) Extract n-grams given a document as the input parameter (functionality from any pre-existing package can be used to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_doc2 = 'work great'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_n_grams(doc, n):\n",
    "    n_grams = []\n",
    "    word_list = doc.split()\n",
    "    if(n>0):\n",
    "        if(len(word_list)<n):\n",
    "            n_grams = ' '.join(word_list)\n",
    "        for i in range(len(word_list)-n):\n",
    "            gram = ' '.join(word_list[i:i+n])\n",
    "            n_grams.append(gram)\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_n_grams(ex_doc, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Works great no odor and',\n",
       " 'great no odor and uses',\n",
       " 'no odor and uses regular',\n",
       " 'odor and uses regular bags',\n",
       " 'and uses regular bags for',\n",
       " 'uses regular bags for us',\n",
       " 'regular bags for us in',\n",
       " 'bags for us in United']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_n_grams(ex_doc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work great'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_n_grams(ex_doc2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining above all things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_docs(docs):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        text = clean_text(doc)\n",
    "        no_stop_text = remove_stopwords(stop_words_file, text)\n",
    "        stem_text = apply_stemming(no_stop_text)\n",
    "        lemma_text = apply_lemmatization(stem_text)\n",
    "        new_docs.append(lemma_text)\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. f) Create a tf-idf matrix from scratch given a corpus as the input parameter (do not use any pre-exisiting packages that offer functionality to do this directly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to add an element in a python dictionary\n",
    "def add_in_dict(_dict, _key):\n",
    "    if _key in _dict.keys():\n",
    "        _dict[_key] +=1\n",
    "    else:\n",
    "        _dict[_key] = 1\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to normalize, lognormalize, and inverse normalize \n",
    "def normalize(_dict, total):\n",
    "    _dict = {k: v / total for k, v in _dict.items()}\n",
    "    return _dict\n",
    "\n",
    "def log_normalize(_dict):\n",
    "    _dict = {k: math.log10(v) for k, v in _dict.items()}\n",
    "    return _dict\n",
    "\n",
    "def inverse_normalize(_dict, total):\n",
    "    _dict = {k: total/v for k, v in _dict.items()}\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to create term frequency and inverse document frequency given a list of docs\n",
    "def create_TF_IDF_dict(docs):\n",
    "    doc_map = {}\n",
    "    TF_dict = {}\n",
    "    IDF_dict = {}\n",
    "    doc_counter = 1\n",
    "    for doc in docs:\n",
    "        doc_map[doc_counter] = doc   # storing <doc_id-doc_text> mapping\n",
    "        word_freq_in_doc = {}   # dictionary to store term freq in doc \n",
    "        for word in doc.split(' '):\n",
    "            IDF_dict = add_in_dict(IDF_dict, word)    # store in global list of terms\n",
    "            word_freq_in_doc = add_in_dict(word_freq_in_doc, word)\n",
    "        TF_dict[doc_counter] = normalize(word_freq_in_doc, len(doc)) # update TF Dictionary\n",
    "        doc_counter +=1\n",
    "    IDF_dict = inverse_normalize(IDF_dict, len(docs))  # normalize idf doctionary\n",
    "    IDF_dict = log_normalize(IDF_dict)  # log normalize idf doctionary\n",
    "    return doc_map, TF_dict, IDF_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create tf-ifd matrix given term frequencies, inverse document frequency and document map.\n",
    "# This is to be consumed along with method `create_TF_IDF_dict`\n",
    "def calculate_TF_IDF_matrix(doc_map, TF_dict, IDF_dict):\n",
    "    mat_tf_idf = pd.DataFrame(index = doc_map.keys(), columns = IDF_dict.keys())\n",
    "    mat_tf_idf = mat_tf_idf.fillna(0)\n",
    "    \n",
    "    for doc in doc_map.keys():\n",
    "        for word in IDF_dict.keys():\n",
    "            tf = 0\n",
    "            tf_vals = TF_dict[doc]\n",
    "            if word in tf_vals.keys():\n",
    "                tf = tf_vals[word]\n",
    "            idf = IDF_dict[word]\n",
    "            mat_tf_idf.loc[doc, word] = tf*idf\n",
    "    print(mat_tf_idf.head())\n",
    "    return np.array(mat_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create tf-idf matrix given a corpus; doClean if set true cleans the documents as well. \n",
    "def create_TF_IDF_matrix(corpus, doClean=False):\n",
    "    corp_lines = get_file_lines(corpus)\n",
    "    if doClean:\n",
    "        corp_lines = clean_docs(corp_lines)\n",
    "    # Assuming each line is a doc\n",
    "    doc_map, TF_dict, IDF_dict = create_TF_IDF_dict(corp_lines)\n",
    "    # print(doc_map, TF_dict, IDF_dict)\n",
    "    # generate TF*IDF values\n",
    "    mat = get_TF_IDF_matrix(doc_map, TF_dict, IDF_dict)\n",
    "    return doc_map, mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. g) Perform k-means clustering on the rows of a matrix, given k and the matrix whose rows are to be clustered as input parameters (implement k-means from scratch and do not use any pre-existing packages that offer functionality to do this directly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec_A, vec_B):\n",
    "    ed = 0\n",
    "    squared_distance = 0\n",
    "    if len(vec_A)==len(vec_B):\n",
    "        for i in range(len(vec_A)):\n",
    "            squared_distance += (vec_A[i]-vec_B[i])**2\n",
    "        ed = math.sqrt(squared_distance)\n",
    "    else:\n",
    "        print(\"Invalid input: vectors of different lengths encountered\")\n",
    "    return ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_centroid(cluster, matrix):\n",
    "    vectors = []\n",
    "    for doc in cluster:\n",
    "        vectors.append(matrix[doc])\n",
    "    return np.mean(vectors, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_mean(matrix, k, num_iterations=100):\n",
    "    cluster_centroids = []\n",
    "    clusters = {}\n",
    "    \n",
    "    for i in range(k):\n",
    "        cluster_centroids.append(matrix[i]) # Let first rows(docs) be intial centroids\n",
    "    \n",
    "    #number of iterations we want to run the algorithm of k means\n",
    "    for _iter in range(num_iterations):\n",
    "        \n",
    "        for i in range(k):\n",
    "            clusters[i] = [] # clusters are empty right now.\n",
    "        \n",
    "        # Assign documents to clusters\n",
    "        for doc in range(len(matrix)):\n",
    "            cluster_dist = []\n",
    "            min_dist = euclidean_distance(cluster_centroids[0], matrix[doc])\n",
    "            clostest_centroid = 0\n",
    "            for centroid in range(1, len(cluster_centroids)):\n",
    "                ed_1 = Euclidean_distance(cluster_centroids[centroid], matrix[doc])\n",
    "                if ed_1<=min_dist:\n",
    "                    clostest_centroid = centroid\n",
    "                    min_dist = ed_1\n",
    "            clusters[clostest_centroid].append(doc)\n",
    "        \n",
    "        # Print updated cluster\n",
    "        # print(clusters)\n",
    "        # print(cluster_centroids)\n",
    "        \n",
    "        # Take average and reset cluster centroids\n",
    "        new_cluster_centroids = []\n",
    "        for i in range(k):\n",
    "            new_cluster_centroids.append(average_centroid(clusters[i], mat))\n",
    "    \n",
    "        # if there is no change in clusters, then pause the iterations\n",
    "        close_flag = True\n",
    "        for i in range(k):\n",
    "            if not (cluster_centroids[i] == new_cluster_centroids[i]).all(): \n",
    "                close_flag = False\n",
    "\n",
    "        # Assign Cluster centroid as new centroids\n",
    "        cluster_centroids = new_cluster_centroids\n",
    "        \n",
    "        if close_flag:\n",
    "            break\n",
    "    \n",
    "    return clusters, cluster_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Write tests for each of the functions above using the Test Corpus provided - assume each new line is a separate document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets test it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        And  you       run       and        to     catch        up      with  \\\n",
      "1  0.004377  0.0  0.003231  0.001616 -0.004071  0.004377  0.003231  0.004377   \n",
      "2  0.000000  0.0  0.000000  0.001710 -0.002154  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.0  0.000000  0.000000 -0.003466  0.000000  0.000000  0.000000   \n",
      "\n",
      "   the       sun    ...        time,     plans      that    either    naught  \\\n",
      "1  0.0  0.001616    ...     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.0  0.001710    ...     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.0  0.000000    ...     0.003728  0.003728  0.003728  0.003728  0.003728   \n",
      "\n",
      "         or      half      page  scribbled  lines.\\n  \n",
      "1  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
      "3  0.003728  0.003728  0.003728   0.003728  0.003728  \n",
      "\n",
      "[3 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "doc_map, mat = create_TF_IDF_matrix('Test Corpus/Test Corpus.txt', doClean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00437726,  0.        ,  0.00323103,  0.00161552, -0.00407062,\n",
       "         0.00437726,  0.00323103,  0.00437726,  0.        ,  0.00161552,\n",
       "         0.00161552,  0.00437726,  0.00437726,  0.00437726,  0.00437726,\n",
       "         0.00161552,  0.00437726,  0.00437726,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.00170962, -0.00215387,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00170962,\n",
       "         0.00170962,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00463225,  0.00170962,\n",
       "         0.00463225,  0.00463225,  0.00170962,  0.00463225,  0.00463225,\n",
       "         0.00463225,  0.00463225,  0.00463225,  0.00170962,  0.00463225,\n",
       "         0.00463225,  0.00463225,  0.00463225,  0.00463225,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        , -0.00346639,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00137571,  0.        ,  0.        ,  0.        ,  0.00137571,\n",
       "         0.        ,  0.        ,  0.00137571,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00137571,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00372751,\n",
       "         0.00372751,  0.00372751,  0.00372751,  0.00372751,  0.00372751,\n",
       "         0.00372751,  0.00372751,  0.00372751,  0.00372751,  0.00372751,\n",
       "         0.00372751,  0.00372751,  0.00372751,  0.00372751,  0.00372751,\n",
       "         0.00372751]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: [0, 2], 1: [1]},\n",
       " [array([ 0.00218863,  0.        ,  0.00161552,  0.00080776, -0.0037685 ,\n",
       "          0.00218863,  0.00161552,  0.00218863,  0.        ,  0.00080776,\n",
       "          0.00080776,  0.00218863,  0.00218863,  0.00218863,  0.00218863,\n",
       "          0.00149561,  0.00218863,  0.00218863,  0.        ,  0.00068786,\n",
       "          0.        ,  0.        ,  0.00068786,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.00068786,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.00186375,\n",
       "          0.00186375,  0.00186375,  0.00186375,  0.00186375,  0.00186375,\n",
       "          0.00186375,  0.00186375,  0.00186375,  0.00186375,  0.00186375,\n",
       "          0.00186375,  0.00186375,  0.00186375,  0.00186375,  0.00186375,\n",
       "          0.00186375]),\n",
       "  array([ 0.        ,  0.        ,  0.        ,  0.00170962, -0.00215387,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.00170962,\n",
       "          0.00170962,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.00463225,  0.00170962,\n",
       "          0.00463225,  0.00463225,  0.00170962,  0.00463225,  0.00463225,\n",
       "          0.00463225,  0.00463225,  0.00463225,  0.00170962,  0.00463225,\n",
       "          0.00463225,  0.00463225,  0.00463225,  0.00463225,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ])])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_mean(mat, 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. \n",
    "### Each of the 2 Main Corpora provided comprises of product reviews from Amazon.com for a pair of products each. \n",
    "### In both corpora, the reviews for each product are separated by three blank lines. \n",
    "### In a Jupyter notebook, use your functions I. a)-g) to treat and cluster the documents in both the Main Corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In both cases use k=2 and assume each new line is a separate document.\n",
    "- Create a 'confusion matrix' A per corpus, whose rows are 1st and 2nd product, whose columns are 1st and 2nd cluster and whose entries Aij are the count of reviews of product i in cluster j.0\n",
    "- Compare the clustering results for both corpora and if you feel the results can be improved, how this can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in consideration\n",
    "text_file_1 = 'Main Corpus 1/Main Corpus 1.txt'\n",
    "text_file_2 = 'Main Corpus 2/Main Corpus 2.txt'\n",
    "\n",
    "# Stopwords file\n",
    "stop_words_file = 'Stopwords/Basic Stopwords List.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. (a)\n",
    "lines_1 = get_file_lines(text_file_1)\n",
    "lines_2 = get_file_lines(text_file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. (b)\n",
    "clean_text(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. (c)\n",
    "remove_stopwords(stop_words_file, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. (d)\n",
    "apply_stemming(doc)\n",
    "apply_lemmatization(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. (e)\n",
    "extract_n_grams(doc, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. (f)\n",
    "_, vector_matrix = create_TF_IDF_matrix(corpus, doClean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. (g)\n",
    "num_of_clusters = 2\n",
    "num_of_iterations = 5\n",
    "k_mean(vector_matrix, num_of_clusters, num_of_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. In a Jupyter notebook, cluster the provided data (Clustering Data) using your k-means implementation. The data cannot be transformed in any way apart from mean-centering.\n",
    "\n",
    "- a) Assess the performance of this k-means implementation for this dataset\n",
    "- b) Discuss the performance and if there is scope for improvement, propose changes to your k-means implementation that would help improve results and explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Code Check list:\n",
    "    1. Use Intention - Revealing Names\n",
    "    2. Pick one word per concept\n",
    "    3. Use Solution/Problem Domain Names\n",
    "    4. Classes Should be Small.\n",
    "    5. Functions should be small.\n",
    "    6. Avoid Duplication.\n",
    "    7. Make sure the code formatting is applied.\n",
    "    8. Use Exceptions rather than return codes.\n",
    "    9. Dont return NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
